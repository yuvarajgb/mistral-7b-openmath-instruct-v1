{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d2d3d5-9a17-4784-8fc8-ce90dbf1dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install dill>=0.4.0 multiprocess>=0.70.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6f8761-5936-4ce2-986d-0d6bf1c85b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install huggingface_hub trl transformers torch datasets bitsandbytes peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d23fd8-8c57-493a-a570-d0c433bae6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17baf39-a8fb-4d09-8ec8-ab915ad879a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your token here (keep it secure!)\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e067fc-d155-43b8-b3f8-02825cb13df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95970d051ed34cc1aa22c81ec219637c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf54a68a08443c4972d70a60bc66b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0: {'problem': 'Ava is planning a camping trip with her friends. She wants to make sure they have enough granola bars for snacks. There will be five people total: Ava, her two friends, and her parents. They will spend 3 days and 2 nights at the campsite, and they plan to have 2 granola bars per person for breakfast and 1 granola bar per person for an afternoon snack each day. How many granola bars will Ava need to pack in total for the entire trip?', 'generated_solution': 'There will be a total of 5 people.\\nEach person needs 2 granola bars for breakfast and 1 granola bar for snack. This amounts to a total of 3 granola bars per person per day.\\nSince the trip is 3 days long, each person will need 3 granola bars/day * 3 days = 9 granola bars.\\nSo for 5 people, Ava will need 5 * 9 = 45 granola bars.\\nThus, Ava will need to pack \\\\boxed{45} granola bars in total for the entire trip.', 'expected_answer': '45', 'problem_source': 'augmented_gsm8k'}\n",
      "Example 1: {'problem': 'Find the sum of all three-digit positive integers whose cubes end with the digits 125.', 'generated_solution': \"To find the sum of all three-digit positive integers whose cubes end with the digits 125, let's analyze how a number's cube can end in 125.\\n\\nA number $n$ whose cube ends in 125 must be of the form $n = 5k$, where $k$ is an integer. This is because the last digit of $n^3$ is determined by the last digit of $n$, and only numbers ending in 5 have cubes that end in 5.\\n\\nNow, consider the last two digits of $n^3$, which are determined by the last two digits of $n$. The only possibilities for the last two digits of $n$ that result in a cube ending in 25 are 25, 75, and 25 + 50 = 75, 75 + 50 = 25 + 100 = 125, etc.\\n\\nThus, the last two digits of $n$ must be 25, 75, or 25 + 50k for some integer k, where 25 + 50k < 1000.\\n\\nSince we're looking for three-digit numbers, the possible values for the hundreds digit of $n$ are 1 through 9.\\n\\nConsidering these constraints, we find the following three-digit numbers whose cubes end with the digits 125:\\n\\n* 125\\n* 375\\n* 625\\n* 875\\n\\nNow, let's find the sum of these numbers:\\n\\\\[ 125 + 375 + 625 + 875 = 500 + 625 + 875 = 1125 + 875 = 2000 \\\\]\\n\\nSo, the sum of all three-digit positive integers whose cubes end with the digits 125 is $\\\\boxed{2000}$.\", 'expected_answer': '2000', 'problem_source': 'augmented_math'}\n",
      "Example 2: {'problem': 'What is the largest number, all of whose digits are 1 or 4, and whose digits add up to 12?', 'generated_solution': \"We need to find the largest number consisting of only 1's and 4's that adds up to 12.\\n\\nThe largest possible number is obtained by using the largest possible number of digits, which is achieved by using as many 1's as possible. However, we also want to maximize the number of digits, so we should use as many 4's as possible without exceeding the sum of 12.\\n\\nThe maximum number of 4's we can use is 3 (since $3 \\\\cdot 4 = 12$), but that would leave no room for 1's. Therefore, let's use 2 fours and make up the rest with 1's.\\n\\nTwo 4's add up to 8, leaving $12 - 8 = 4$ to be made up by 1's. We can use 4 ones to make up this difference.\\n\\nThe largest number is thus obtained by arranging these digits in decreasing order: 441111.\\n\\nHowever, we want the largest number, so we should arrange the digits in decreasing order. Thus, the largest number is 444111, no, there are too many 4's.\\n\\nSo, the largest number is:\\n\\\\[ \\\\boxed{441111} \\\\]\", 'expected_answer': '441111', 'problem_source': 'augmented_math'}\n",
      "Example 3: {'problem': 'Two distinct primes, each greater than 20, are multiplied. What is the least possible product of these two primes?', 'generated_solution': 'To find the least possible product of two distinct primes greater than 20, we need to find the smallest two distinct primes greater than 20.\\n\\nPrimes greater than 20 are: 23, 29, 31,...\\n\\nThe two smallest distinct primes greater than 20 are 23 and 29.\\n\\nTheir product is:\\n\\\\[ 23 \\\\times 29 = 667 \\\\]\\n\\nTherefore, the least possible product of these two primes is $\\\\boxed{667}$', 'expected_answer': '667', 'problem_source': 'math'}\n",
      "Example 4: {'problem': 'A farmer has 100 meters of fencing and wants to enclose a rectangular garden with an area of 500 square meters. If the farmer also wants to leave a 5-meter wide path around the entire garden, how many meters of fencing will be needed to enclose the garden and the path? Express your answer as a decimal to the nearest tenth.', 'generated_solution': \"Let's denote the length of the inner rectangular garden as $L$ and the width as $W$. We know that the area of the inner garden is 500 square meters, so we have the equation:\\n\\\\[ LW = 500 \\\\]\\n\\nThe farmer wants to leave a 5-meter wide path around the entire garden. This means that the outer dimensions of the garden and the path will be $(L+10)$ by $(W+10)$.\\n\\nThe perimeter of the outer rectangle (garden and path) is the total length of fencing needed. The perimeter $P$ of a rectangle is given by $P = 2L + 2W$. For the outer rectangle, this becomes:\\n\\\\[ P = 2(L+10) + 2(W+10) \\\\]\\n\\nWe can simplify this expression:\\n\\\\[ P = 2L + 20 + 2W + 20 \\\\]\\n\\\\[ P = 2L + 2W + 40 \\\\]\\n\\nWe already know that the farmer has 100 meters of fencing, but we need to find out how much fencing is actually needed for the garden and the path.\\n\\nGiven the area of the inner garden, $LW = 500$, we can express one variable in terms of the other, for example, $W = \\\\frac{500}{L}$.\\n\\nHowever, to minimize the amount of fencing used, the farmer should make the garden as close to a square as possible, because a square has the smallest perimeter for a given area. This means $L$ should be as close to $W$ as possible.\\n\\nSince $LW = 500$, and we want $L$ to be close to $W$, let's find the square root of 500, which will give us a value close to both $L$ and $W$:\\n\\\\[ \\\\sqrt{500} \\\\approx 22.36 \\\\]\\n\\nThis means the dimensions of the inner garden that would use the least amount of fencing for the outer path would be approximately 22.36 meters by 22.36 meters.\\n\\nNow, calculate the perimeter of the outer rectangle (garden and path) using these dimensions:\\n\\\\[ P = 2(22.36 + 10) + 2(22.36 + 10) \\\\]\\n\\\\[ P = 2(32.36) + 2(32.36) \\\\]\\n\\\\[ P = 64.72 + 64.72 \\\\]\\n\\\\[ P = 129.44 \\\\]\\n\\nRounded to the nearest tenth, the total length of fencing needed to enclose the garden and the path is:\\n\\\\[ \\\\boxed{129.4} \\\\]\", 'expected_answer': '129.4', 'problem_source': 'augmented_math'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"nvidia/OpenMathInstruct-2\", streaming=True, split=\"train\")\n",
    "for i, example in enumerate(dataset.take(5)):\n",
    "    print(f\"Example {i}: {example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ffc4a4-047d-406b-8895-d25115dd38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset, IterableDataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import DPOTrainer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Accelerator\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96a524e-0a84-46b3-adb3-d8319f6af767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_dataset():\n",
    "    try:\n",
    "        # Verify file existence\n",
    "        if not os.path.exists(\"OpenMathInstruct-2.json\"):\n",
    "            raise FileNotFoundError(\"OpenMathInstruct-2.json not found in working directory\")\n",
    "        dataset = load_dataset(\"json\", data_files={\"train\": \"OpenMathInstruct-2.json\"})[\"train\"]\n",
    "        logger.info(\"Loaded local JSON dataset\")\n",
    "\n",
    "        # Clean dataset (non-streaming)\n",
    "        def clean_dataset(dataset):\n",
    "            seen = set()\n",
    "            cleaned = []\n",
    "            for i, example in enumerate(dataset):\n",
    "                if not isinstance(example, dict):\n",
    "                    logger.warning(f\"Skipping invalid example at index {i}: not a dictionary ({type(example)})\")\n",
    "                    continue\n",
    "                # Handle typo in key\n",
    "                if \"expected_answe\" in example:\n",
    "                    example[\"expected_answer\"] = example.pop(\"expected_answe\")\n",
    "                # Check required keys\n",
    "                if \"problem\" not in example or \"generated_solution\" not in example:\n",
    "                    logger.warning(f\"Skipping example at index {i}: missing keys ({example})\")\n",
    "                    continue\n",
    "                prompt = example[\"problem\"]\n",
    "                if not isinstance(prompt, str) or len(prompt.strip()) < 10:\n",
    "                    logger.warning(f\"Skipping example at index {i}: invalid prompt ({type(prompt)})\")\n",
    "                    continue\n",
    "                if prompt not in seen:\n",
    "                    seen.add(prompt)\n",
    "                    cleaned.append({\n",
    "                        \"prompt\": example[\"problem\"],\n",
    "                        \"chosen\": example[\"generated_solution\"],\n",
    "                        \"expected_answer\": example.get(\"expected_answer\", \"\")\n",
    "                    })\n",
    "                else:\n",
    "                    logger.info(f\"Skipping duplicate prompt at index {i}: {prompt[:50]}...\")\n",
    "            if not cleaned:\n",
    "                logger.error(\"No valid examples after cleaning!\")\n",
    "                raise ValueError(\"Empty dataset after cleaning\")\n",
    "            return Dataset.from_list(cleaned)\n",
    "\n",
    "        dataset = clean_dataset(dataset)\n",
    "        logger.info(f\"Cleaned dataset: {len(dataset)} examples\")\n",
    "        dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        return dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load local JSON: {str(e)}\")\n",
    "        try:\n",
    "            dataset = load_dataset(\"nvidia/OpenMathInstruct-2\", streaming=True, split=\"train\")\n",
    "            logger.info(\"Loaded OpenMathInstruct-2 in streaming mode\")\n",
    "\n",
    "            # Normalize streaming dataset keys\n",
    "            def normalize_keys(example):\n",
    "                if not isinstance(example, dict) or \"problem\" not in example or \"generated_solution\" not in example:\n",
    "                    logger.warning(f\"Skipping example with missing keys: {example}\")\n",
    "                    return None\n",
    "                return {\n",
    "                    \"prompt\": example[\"problem\"],\n",
    "                    \"chosen\": example[\"generated_solution\"],\n",
    "                    \"expected_answer\": example.get(\"expected_answer\", \"\")\n",
    "                }\n",
    "\n",
    "            # Filter out None values and normalize\n",
    "            dataset = dataset.map(normalize_keys).filter(lambda x: x is not None)\n",
    "\n",
    "            # Create validation dataset\n",
    "            val_size = 10000\n",
    "            train_dataset = dataset.skip(val_size)\n",
    "            val_dataset = dataset.take(val_size)\n",
    "            # Cache validation dataset to disk\n",
    "            val_dataset = Dataset.from_list(list(val_dataset))\n",
    "            logger.info(f\"Created validation dataset with {len(val_dataset)} examples\")\n",
    "            return train_dataset, val_dataset\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load Hugging Face dataset: {str(e)}\")\n",
    "            dataset = Dataset.from_list([\n",
    "                {\n",
    "                    \"prompt\": \"Solve: What is the sum of the first 5 positive integers?\",\n",
    "                    \"chosen\": \"Use the formula: sum = n(n+1)/2. For n=5, sum = 5(6)/2 = 15. Final answer: 15.\",\n",
    "                    \"expected_answer\": \"15\"\n",
    "                } for _ in range(1000)\n",
    "            ])\n",
    "            logger.info(\"Using dummy dataset\")\n",
    "            dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "            return dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3979b1ee-e274-4da4-8e33-4e6b31541b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff0096c5b2a49b78d5bf3ad64923f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c7c371ddd44011a447df3e2360cd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070a2932c439463fba617d9fb55ec400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766a00d7cace4202b8e6f0a2fa48e279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94adb754ecf64151a0ce69a086f3a411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9321d5b8b61844cc893e919296f0b445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef2782b8f224a948c69786a60f1e57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acf860afcc546978122794e8f02359c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ef0447a0d14c63958d82625a4daa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71af3759eb0d4c5e9940639dfa587008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d69422b938c41c797c6231d6379ec97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Failed to load local JSON: OpenMathInstruct-2.json not found in working directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683669ba61ce4ac190705d56a939ab69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e227fe1336347f4bd63c3abc4618576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8b4dbd57bb436db4a6995189087966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded OpenMathInstruct-2 in streaming mode\n",
      "INFO:__main__:Created validation dataset with 10000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5363cb9502411cbde7f60ee2ac4cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 2:51:23, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.166100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('mistral-7b-math/sft/final/tokenizer_config.json',\n",
       " 'mistral-7b-math/sft/final/special_tokens_map.json',\n",
       " 'mistral-7b-math/sft/final/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def augment_example(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    chosen = example[\"chosen\"]\n",
    "    numbers = re.findall(r'\\d+', chosen)\n",
    "    rejected = chosen\n",
    "    if numbers:\n",
    "        wrong_number = str(int(numbers[-1]) + random.randint(1, 10))\n",
    "        rejected = chosen.replace(numbers[-1], wrong_number)\n",
    "    return {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "def tokenize_sft(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    chosen = example[\"chosen\"]\n",
    "    # Use the same max_length for both prompt and chosen\n",
    "    inputs = tokenizer(prompt, truncation=True, max_length=512, padding=\"max_length\")\n",
    "    labels = tokenizer(chosen, truncation=True, max_length=512, padding=\"max_length\")\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# SFT Training\n",
    "train_dataset, val_dataset = load_and_clean_dataset()\n",
    "\n",
    "# Handle streaming dataset\n",
    "if isinstance(train_dataset, IterableDataset):\n",
    "    train_dataset = train_dataset.map(tokenize_sft, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_sft, batched=True)\n",
    "else:\n",
    "    train_dataset = train_dataset.map(tokenize_sft, batched=True, remove_columns=[\"prompt\", \"chosen\", \"expected_answer\"])\n",
    "    val_dataset = val_dataset.map(tokenize_sft, batched=True, remove_columns=[\"prompt\", \"chosen\", \"expected_answer\"])\n",
    "\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=\"mistral-7b-math/sft/\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    max_steps=1000,  # Limit to 1000 steps as specified\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    #eval_steps=500,\n",
    "    #save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    #load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "sft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=sft_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "sft_trainer.train()\n",
    "\n",
    "# Save SFT model\n",
    "model.save_pretrained(\"mistral-7b-math/sft/final/\")\n",
    "tokenizer.save_pretrained(\"mistral-7b-math/sft/final/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "666840b8-36cf-4b41-b64d-2be1cd665964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Pre-filter invalid examples\n",
    "def is_valid_example(example):\n",
    "    if not isinstance(example, dict):\n",
    "        logger.warning(f\"Invalid example: not a dictionary - {example}\")\n",
    "        return False\n",
    "    prompt = example.get(\"prompt\")\n",
    "    chosen = example.get(\"chosen\")\n",
    "    if prompt is None or chosen is None:\n",
    "        logger.warning(f\"Invalid example: missing prompt/chosen - {example}\")\n",
    "        return False\n",
    "    if not isinstance(prompt, str) or not isinstance(chosen, str):\n",
    "        logger.warning(f\"Invalid example: non-string prompt/chosen - {example}\")\n",
    "        return False\n",
    "    if len(prompt.strip()) < 10 or len(chosen.strip()) < 10:\n",
    "        logger.warning(f\"Invalid example: empty/short prompt/chosen - {example}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Normalize dataset keys\n",
    "def normalize_keys(example):\n",
    "    if not isinstance(example, dict):\n",
    "        logger.warning(f\"Invalid raw example: not a dictionary - {example}\")\n",
    "        return None\n",
    "    if \"problem\" not in example or \"generated_solution\" not in example:\n",
    "        logger.warning(f\"Invalid raw example: missing keys - {example}\")\n",
    "        return None\n",
    "    prompt = example[\"problem\"]\n",
    "    chosen = example[\"generated_solution\"]\n",
    "    if not isinstance(prompt, str) or not isinstance(chosen, str):\n",
    "        logger.warning(f\"Invalid raw example: non-string prompt/chosen - {example}\")\n",
    "        return None\n",
    "    if len(prompt.strip()) < 10 or len(chosen.strip()) < 10:\n",
    "        logger.warning(f\"Invalid raw example: empty/short prompt/chosen - {example}\")\n",
    "        return None\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"expected_answer\": str(example.get(\"expected_answer\", \"\"))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "560092b3-422d-4662-b94f-d0c67b1ce227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading OpenMathInstruct-2 in streaming mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8be28f142b94becb13f6e30333db7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b225ff08ec0d4ea6bbedba83986e1d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processed 10000 examples, collected 10000 valid\n",
      "INFO:__main__:Processed 20000 examples, collected 20000 valid\n",
      "INFO:__main__:Processed 30000 examples, collected 30000 valid\n",
      "INFO:__main__:Processed 40000 examples, collected 40000 valid\n",
      "INFO:__main__:Processed 50000 examples, collected 50000 valid\n",
      "INFO:__main__:Processed 60000 examples, collected 60000 valid\n",
      "INFO:__main__:Processed 70000 examples, collected 70000 valid\n",
      "INFO:__main__:Processed 80000 examples, collected 80000 valid\n",
      "INFO:__main__:Processed 90000 examples, collected 90000 valid\n",
      "INFO:__main__:Processed 100000 examples, collected 100000 valid\n",
      "INFO:__main__:Collected 100000 valid examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000 samples. Train: 80000, Eval: 20000\n"
     ]
    }
   ],
   "source": [
    "# Load and process OpenMathInstruct-2\n",
    "def load_and_split_dataset(max_samples=100000):\n",
    "    logger.info(\"Loading OpenMathInstruct-2 in streaming mode...\")\n",
    "    dataset = load_dataset(\"nvidia/OpenMathInstruct-2\", streaming=True, split=\"train\")\n",
    "\n",
    "    # Collect valid examples\n",
    "    valid_examples = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        normalized = normalize_keys(example)\n",
    "        if normalized and is_valid_example(normalized):\n",
    "            valid_examples.append(normalized)\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            logger.info(f\"Processed {i + 1} examples, collected {len(valid_examples)} valid\")\n",
    "\n",
    "    logger.info(f\"Collected {len(valid_examples)} valid examples\")\n",
    "\n",
    "    # Shuffle examples\n",
    "    random.shuffle(valid_examples)\n",
    "\n",
    "    # Split into train and eval\n",
    "    train_size = int(0.8 * len(valid_examples))\n",
    "    train_data = valid_examples[:train_size]\n",
    "    eval_data = valid_examples[train_size:]\n",
    "\n",
    "    return train_data, eval_data\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and split dataset\n",
    "    train_data, eval_data = load_and_split_dataset(max_samples=100000)\n",
    "\n",
    "    # Save to JSON files\n",
    "    with open(\"train_synthetic_math.json\", \"w\") as f:\n",
    "        json.dump(train_data, f, indent=2)\n",
    "    with open(\"eval_synthetic_math.json\", \"w\") as f:\n",
    "        json.dump(eval_data, f, indent=2)\n",
    "\n",
    "    print(f\"Generated {len(train_data) + len(eval_data)} samples. Train: {len(train_data)}, Eval: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04063bb6-090e-4f9a-8707-ab60828cd937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ce2e1-04ef-4bae-a6a6-4c763a8359a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1315f0fdfb2f4fb1a6b0f8037a843c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8a2a05cfda4e88addf35f9a2dbfc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0589f052844440da81bdb0eba4a152e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116f33a3d9584331a50f8ca060a83f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e784de96b8ba453184b94f3a17638dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac12ac53ed64c2885ab8158b2046f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fc64d846d64193a8c5531832a6925b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading SFT LoRA adapters from mistral-7b-math/sft/final...\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Merged SFT LoRA adapters successfully\n",
      "INFO:__main__:GPU memory allocated: 4.66 GB\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "INFO:__main__:Trainable parameters: 13631488 / 3765702656 (0.36%)\n",
      "INFO:__main__:Loading train_synthetic_math.json...\n",
      "INFO:__main__:Loaded 80000 examples, filtered to 80000 valid examples\n",
      "INFO:__main__:Created DPO dataset with 10000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1911d6784a49759d47f7229560b883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Tokenized DPO dataset with 10000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fcbda5d66140368ee40a77af307a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36af79045e04e0daf5870b17288c5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3436cc6938644aaeaa3798ef3a91811e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "INFO:__main__:Evaluating SFT model before DPO...\n",
      "INFO:__main__:Loading eval_synthetic_math.json...\n",
      "INFO:__main__:Loaded 20000 evaluation examples\n",
      "INFO:__main__:Evaluation accuracy: 0.13\n",
      "INFO:__main__:SFT model accuracy: 0.13\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 5:35:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.072700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "INFO:__main__:Evaluating DPO model...\n",
      "INFO:__main__:Loading eval_synthetic_math.json...\n",
      "INFO:__main__:Loaded 20000 evaluation examples\n",
      "INFO:__main__:Evaluation accuracy: 0.00\n",
      "INFO:__main__:DPO model accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import logging\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Validate example\n",
    "def is_valid_example(example):\n",
    "    if not isinstance(example, dict):\n",
    "        logger.warning(f\"Invalid example: not a dictionary - {example}\")\n",
    "        return False\n",
    "    prompt = example.get(\"prompt\")\n",
    "    chosen = example.get(\"chosen\")\n",
    "    if prompt is None or chosen is None:\n",
    "        logger.warning(f\"Invalid example: missing prompt/chosen - {example}\")\n",
    "        return False\n",
    "    if not isinstance(prompt, str) or not isinstance(chosen, str):\n",
    "        logger.warning(f\"Invalid example: non-string prompt/chosen - {example}\")\n",
    "        return False\n",
    "    if len(prompt.strip()) < 10 or len(chosen.strip()) < 10:\n",
    "        logger.warning(f\"Invalid example: empty/short prompt/chosen - {example}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Load datasets\n",
    "def load_dpo_dataset():\n",
    "    logger.info(\"Loading train_synthetic_math.json...\")\n",
    "    try:\n",
    "        df = pd.read_json(\"train_synthetic_math.json\")\n",
    "        data = df.to_dict(orient=\"records\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Pandas failed: {str(e)}. Falling back to json.load...\")\n",
    "        with open(\"train_synthetic_math.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    \n",
    "    valid_data = [item for item in data if is_valid_example(item)]\n",
    "    logger.info(f\"Loaded {len(data)} examples, filtered to {len(valid_data)} valid examples\")\n",
    "\n",
    "    max_samples = 10000\n",
    "    dpo_data = []\n",
    "    for example in valid_data[:min(max_samples, len(valid_data))]:\n",
    "        correct_response = example[\"chosen\"]\n",
    "        numbers = re.findall(r'\\d+', correct_response)\n",
    "        incorrect_response = correct_response\n",
    "        if numbers:\n",
    "            wrong_number = str(int(numbers[-1]) + random.randint(1, 10))\n",
    "            incorrect_response = correct_response.replace(numbers[-1], wrong_number)\n",
    "        else:\n",
    "            incorrect_response = correct_response + \" (incorrect)\"\n",
    "\n",
    "        dpo_data.append({\n",
    "            \"prompt\": example[\"prompt\"],\n",
    "            \"chosen\": correct_response,\n",
    "            \"rejected\": incorrect_response\n",
    "        })\n",
    "\n",
    "    dpo_dataset = Dataset.from_list(dpo_data)\n",
    "    logger.info(f\"Created DPO dataset with {len(dpo_dataset)} examples\")\n",
    "\n",
    "    return dpo_dataset\n",
    "\n",
    "# Model and Tokenizer Setup\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "sft_model_path = \"mistral-7b-math/sft/final\"\n",
    "\n",
    "# Load tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(sft_model_path, padding_side=\"right\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Failed to load tokenizer from {sft_model_path}: {str(e)}. Using {base_model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side=\"right\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    logger.info(f\"Set tokenizer pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Load base model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load SFT LoRA adapters and merge\n",
    "try:\n",
    "    logger.info(f\"Loading SFT LoRA adapters from {sft_model_path}...\")\n",
    "    model = PeftModel.from_pretrained(model, sft_model_path)\n",
    "    model = model.merge_and_unload()  # Merge adapters into base model\n",
    "    logger.info(\"Merged SFT LoRA adapters successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load or merge SFT LoRA adapters: {str(e)}. Using base model...\")\n",
    "    # Continue with base model if SFT loading fails\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.gradient_checkpointing_enable()\n",
    "logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# Apply LoRA for DPO\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Verify trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params/total_params*100:.2f}%)\")\n",
    "if trainable_params == 0:\n",
    "    raise ValueError(\"No trainable parameters found. Check LoRA configuration and model loading.\")\n",
    "\n",
    "# Load DPO dataset\n",
    "dpo_dataset = load_dpo_dataset()\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_dpo(examples):\n",
    "    prompt_input_ids = []\n",
    "    prompt_attention_mask = []\n",
    "    chosen_input_ids = []\n",
    "    chosen_attention_mask = []\n",
    "    rejected_input_ids = []\n",
    "    rejected_attention_mask = []\n",
    "    for prompt, choice, reject in zip(examples[\"prompt\"], examples[\"chosen\"], examples[\"rejected\"]):\n",
    "        try:\n",
    "            prompt_tokens = tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            chosen_tokens = tokenizer(\n",
    "                choice,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            rejected_tokens = tokenizer(\n",
    "                reject,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            prompt_input_ids.append(prompt_tokens[\"input_ids\"][0])\n",
    "            prompt_attention_mask.append(prompt_tokens[\"attention_mask\"][0])\n",
    "            chosen_input_ids.append(chosen_tokens[\"input_ids\"][0])\n",
    "            chosen_attention_mask.append(chosen_tokens[\"attention_mask\"][0])\n",
    "            rejected_input_ids.append(rejected_tokens[\"input_ids\"][0])\n",
    "            rejected_attention_mask.append(rejected_tokens[\"attention_mask\"][0])\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping example due to tokenization error: {str(e)}\")\n",
    "            prompt_input_ids.append(torch.zeros(512, dtype=torch.long))\n",
    "            prompt_attention_mask.append(torch.zeros(512, dtype=torch.long))\n",
    "            chosen_input_ids.append(torch.zeros(512, dtype=torch.long))\n",
    "            chosen_attention_mask.append(torch.zeros(512, dtype=torch.long))\n",
    "            rejected_input_ids.append(torch.zeros(512, dtype=torch.long))\n",
    "            rejected_attention_mask.append(torch.zeros(512, dtype=torch.long))\n",
    "    return {\n",
    "        \"prompt_input_ids\": prompt_input_ids,\n",
    "        \"prompt_attention_mask\": prompt_attention_mask,\n",
    "        \"chosen_input_ids\": chosen_input_ids,\n",
    "        \"chosen_attention_mask\": chosen_attention_mask,\n",
    "        \"rejected_input_ids\": rejected_input_ids,\n",
    "        \"rejected_attention_mask\": rejected_attention_mask\n",
    "    }\n",
    "\n",
    "dpo_dataset = dpo_dataset.map(tokenize_dpo, batched=True)\n",
    "logger.info(f\"Tokenized DPO dataset with {len(dpo_dataset)} examples\")\n",
    "\n",
    "# DPO Configuration\n",
    "dpo_args = DPOConfig(\n",
    "    output_dir=\"./mistral-7b-math-dpo\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "# DPO training\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=dpo_args,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "# Evaluate on eval_synthetic_math.json\n",
    "def evaluate_model():\n",
    "    logger.info(\"Loading eval_synthetic_math.json...\")\n",
    "    try:\n",
    "        df = pd.read_json(\"eval_synthetic_math.json\")\n",
    "        data = df.to_dict(orient=\"records\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Pandas failed: {str(e)}. Falling back to json.load...\")\n",
    "        with open(\"eval_synthetic_math.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    \n",
    "    valid_data = [item for item in data if is_valid_example(item)]\n",
    "    eval_dataset = Dataset.from_list(valid_data)\n",
    "    logger.info(f\"Loaded {len(eval_dataset)} evaluation examples\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    num_samples = min(100, len(eval_dataset))\n",
    "    for i, example in enumerate(eval_dataset.select(range(num_samples))):\n",
    "        prompt = example[\"prompt\"]\n",
    "        expected_answer = example[\"expected_answer\"]\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda:0\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract final answer from generated text\n",
    "        generated_numbers = re.findall(r'\\d+', generated)\n",
    "        generated_answer = generated_numbers[-1] if generated_numbers else \"\"\n",
    "        \n",
    "        # Normalize and compare\n",
    "        if generated_answer == expected_answer.strip():\n",
    "            correct += 1\n",
    "        else:\n",
    "            logger.debug(f\"Example {i}: Expected {expected_answer}, Got {generated_answer}\")\n",
    "            logger.debug(f\"Generated: {generated[:200]}...\")\n",
    "\n",
    "    accuracy = correct / num_samples if num_samples > 0 else 0\n",
    "    logger.info(f\"Evaluation accuracy: {accuracy:.2f}\")\n",
    "    return accuracy\n",
    "\n",
    "logger.info(\"Evaluating SFT model before DPO...\")\n",
    "sft_accuracy = evaluate_model()\n",
    "logger.info(f\"SFT model accuracy: {sft_accuracy:.2f}\")\n",
    "\n",
    "# Train and evaluate\n",
    "dpo_trainer.train()\n",
    "model.save_pretrained(\"./mistral-7b-math-dpo/final\")\n",
    "tokenizer.save_pretrained(\"./mistral-7b-math-dpo/final\")\n",
    "\n",
    "logger.info(\"Evaluating DPO model...\")\n",
    "dpo_accuracy = evaluate_model()\n",
    "logger.info(f\"DPO model accuracy: {dpo_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bd0cd-758a-412b-888d-ceb73490e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92926fa6-1780-4c37-960e-f63fe1f00b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading base model on GPU with 4-bit quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca83b3161fe43aa95ed1cf3ea6492bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading SFT LoRA adapters from mistral-7b-math/sft/final...\n",
      "INFO:__main__:Loaded SFT LoRA adapters successfully\n",
      "INFO:__main__:peft_config after loading: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='mistralai/Mistral-7B-Instruct-v0.1', revision=None, inference_mode=True, r=16, target_modules={'q_proj', 'v_proj', 'k_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "INFO:__main__:Active adapter: default\n",
      "INFO:__main__:Trainable parameters: 13631488 / 3765702656 (0.36%)\n",
      "INFO:__main__:CPU memory usage: 1.38 GB\n",
      "INFO:__main__:GPU memory allocated: 4.71 GB\n",
      "INFO:__main__:GPU memory reserved: 7.47 GB\n",
      "INFO:__main__:Loading evaluation dataset from MATH dataset.xlsx...\n",
      "INFO:__main__:Loaded 12500 examples, filtered to 12500 valid examples\n",
      "INFO:__main__:Evaluating SFT model on 1000 samples with batch_size=8...\n",
      "Evaluating batches:   7%|         | 9/125 [03:29<43:51, 22.68s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  15%|        | 19/125 [07:11<39:26, 22.32s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  23%|       | 29/125 [10:57<35:24, 22.13s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  31%|       | 39/125 [14:43<32:36, 22.75s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  39%|      | 49/125 [18:36<29:27, 23.26s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  47%|     | 59/125 [22:30<24:53, 22.63s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  55%|    | 69/125 [26:20<21:32, 23.08s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  63%|   | 79/125 [30:06<17:40, 23.06s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  71%|   | 89/125 [34:05<14:23, 24.00s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  79%|  | 99/125 [37:59<10:01, 23.15s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  87%| | 109/125 [41:52<05:59, 22.46s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches:  95%|| 119/125 [45:45<02:16, 22.71s/it]INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.42 GB\n",
      "Evaluating batches: 100%|| 125/125 [48:03<00:00, 23.07s/it]\n",
      "INFO:__main__:SFT model accuracy: 0.03 (28/1000)\n",
      "INFO:__main__:Final SFT model accuracy: 0.03\n",
      "INFO:__main__:CPU memory usage: 1.65 GB\n",
      "INFO:__main__:GPU memory allocated: 4.72 GB\n",
      "INFO:__main__:GPU memory reserved: 7.41 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "import re\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to monitor memory usage\n",
    "def log_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    logger.info(f\"CPU memory usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        logger.info(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# Validate example\n",
    "def is_valid_example(example):\n",
    "    if not isinstance(example, dict):\n",
    "        logger.warning(f\"Invalid example: not a dictionary - {example}\")\n",
    "        return False\n",
    "    prompt = example.get(\"problem\")\n",
    "    solution = example.get(\"solution\")\n",
    "    if prompt is None or solution is None:\n",
    "        logger.warning(f\"Invalid example: missing problem/solution - {example}\")\n",
    "        return False\n",
    "    if not isinstance(prompt, str) or not isinstance(solution, str):\n",
    "        logger.warning(f\"Invalid example: non-string problem/solution - {example}\")\n",
    "        return False\n",
    "    if len(prompt.strip()) < 10 or len(solution.strip()) < 10:\n",
    "        logger.warning(f\"Invalid example: empty/short problem/solution - {example}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Load evaluation dataset\n",
    "def load_eval_dataset(file_path=\"MATH dataset.xlsx\"):\n",
    "    logger.info(f\"Loading evaluation dataset from {file_path}...\")\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        data = df.to_dict(orient=\"records\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load Excel file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    valid_data = [item for item in data if is_valid_example(item)]\n",
    "    logger.info(f\"Loaded {len(data)} examples, filtered to {len(valid_data)} valid examples\")\n",
    "    return valid_data\n",
    "\n",
    "# Model and Tokenizer Setup\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "sft_model_path = \"mistral-7b-math/sft/final\"\n",
    "\n",
    "# Load tokenizer with left padding\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(sft_model_path, padding_side=\"left\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Failed to load tokenizer from {sft_model_path}: {str(e)}. Using {base_model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side=\"left\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    logger.info(f\"Set tokenizer pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Load base model on GPU with 4-bit quantization\n",
    "logger.info(\"Loading base model on GPU with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare for k-bit training\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Load SFT LoRA adapters\n",
    "try:\n",
    "    logger.info(f\"Loading SFT LoRA adapters from {sft_model_path}...\")\n",
    "    model = PeftModel.from_pretrained(model, sft_model_path)\n",
    "    logger.info(\"Loaded SFT LoRA adapters successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load SFT LoRA adapters: {str(e)}. Using base model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:0\",\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Debug peft_config and active_adapter\n",
    "if hasattr(model, 'peft_config'):\n",
    "    logger.info(f\"peft_config after loading: {model.peft_config}\")\n",
    "    logger.info(f\"Active adapter: {model.active_adapter}\")\n",
    "else:\n",
    "    logger.error(\"peft_config is missing after loading SFT model.\")\n",
    "    raise ValueError(\"peft_config is missing. Check SFT model directory and loading process.\")\n",
    "\n",
    "# Ensure LoRA parameters are trainable (for future DPO training)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Trainable parameters: {trainable_params} / {total_params} ({trainable_params/total_params*100:.2f}%)\")\n",
    "\n",
    "# Disable gradient checkpointing for evaluation (inference only)\n",
    "model.gradient_checkpointing_disable()\n",
    "log_memory_usage()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_sft_model(data, max_samples=1000, batch_size=8):\n",
    "    logger.info(f\"Evaluating SFT model on {max_samples} samples with batch_size={batch_size}...\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    num_samples = min(max_samples, len(data))\n",
    "\n",
    "    # Process in batches\n",
    "    for batch_start in tqdm(range(0, num_samples, batch_size), desc=\"Evaluating batches\"):\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "        batch_data = data[batch_start:batch_end]\n",
    "\n",
    "        # Prepare batch inputs\n",
    "        prompts = [example[\"problem\"] for example in batch_data]\n",
    "        expected_solutions = [example[\"solution\"] for example in batch_data]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(\"cuda:0\")\n",
    "\n",
    "        # Generate responses for batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,  # Reduced for MATH problems\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode generated outputs\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        # Extract answers (supports CoT)\n",
    "        def extract_answer(text):\n",
    "            # Try LaTeX boxed answer\n",
    "            boxed = re.search(r'\\\\boxed\\{([^}]+)\\}', text)\n",
    "            if boxed:\n",
    "                return boxed.group(1).strip()\n",
    "            # Try final answer pattern\n",
    "            final_answer = re.search(r'(?:Final Answer|Answer):?\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "            if final_answer:\n",
    "                return final_answer.group(1).strip()\n",
    "            # Try last number\n",
    "            numbers = re.findall(r'\\d+', text)\n",
    "            return numbers[-1] if numbers else \"\"\n",
    "\n",
    "        # Evaluate batch\n",
    "        for idx, (generated, expected_solution) in enumerate(zip(generated_texts, expected_solutions)):\n",
    "            global_idx = batch_start + idx\n",
    "            generated_answer = extract_answer(generated)\n",
    "            expected_answer = extract_answer(expected_solution)\n",
    "\n",
    "            # Log for debugging\n",
    "            logger.debug(f\"Example {global_idx}: Prompt: {prompts[idx][:100]}...\")\n",
    "            logger.debug(f\"Expected Solution: {expected_solution[:200]}...\")\n",
    "            logger.debug(f\"Expected Answer: {expected_answer}\")\n",
    "            logger.debug(f\"Generated: {generated[:200]}...\")\n",
    "            logger.debug(f\"Generated Answer: {generated_answer}\")\n",
    "\n",
    "            # Compare answers\n",
    "            if generated_answer == expected_answer:\n",
    "                correct += 1\n",
    "\n",
    "        # Log memory usage periodically\n",
    "        if (batch_start // batch_size + 1) % 10 == 0:\n",
    "            log_memory_usage()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = correct / num_samples if num_samples > 0 else 0\n",
    "    logger.info(f\"SFT model accuracy: {accuracy:.2f} ({correct}/{num_samples})\")\n",
    "    return accuracy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    eval_data = load_eval_dataset(\"MATH dataset.xlsx\")\n",
    "\n",
    "    # Evaluate\n",
    "    sft_accuracy = evaluate_sft_model(eval_data, max_samples=1000, batch_size=8)\n",
    "    logger.info(f\"Final SFT model accuracy: {sft_accuracy:.2f}\")\n",
    "    log_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3961314f-287d-46a7-b043-07b312a94586",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4fc7d6-3969-4a64-b159-f6d429a36706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading tokenizer from mistralai/Mistral-7B-Instruct-v0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80bf5655b5c4bdab722236fb75aad11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b88653261647d085be96209f0714da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01b706a1d5f4683bff05b0cf79e99db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43e9b28dd3c46d7ab5f8a7f0cd11637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Set tokenizer pad_token to eos_token: </s>\n",
      "INFO:__main__:Loading base model on GPU with 4-bit quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459b7ef415df46fea1877e650bd09f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total parameters: 3752071168\n",
      "INFO:__main__:CPU memory usage: 2.11 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.77 GB\n",
      "INFO:__main__:Loading evaluation dataset from MATH dataset.xlsx...\n",
      "INFO:__main__:Loaded 12500 examples, filtered to 12500 valid examples\n",
      "INFO:__main__:Evaluating base model on 1000 samples with batch_size=8...\n",
      "Evaluating batches:   7%|         | 9/125 [00:58<11:51,  6.14s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  15%|        | 19/125 [01:47<11:07,  6.30s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  23%|       | 29/125 [02:46<09:55,  6.20s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  31%|       | 39/125 [03:51<09:14,  6.44s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  39%|      | 49/125 [04:58<09:54,  7.82s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  47%|     | 59/125 [06:13<07:24,  6.74s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  55%|    | 69/125 [07:07<04:29,  4.82s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  63%|   | 79/125 [08:12<05:03,  6.60s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  71%|   | 89/125 [09:16<04:20,  7.24s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  79%|  | 99/125 [10:17<02:05,  4.81s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  87%| | 109/125 [11:25<01:07,  4.21s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches:  95%|| 119/125 [12:21<00:33,  5.63s/it]INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n",
      "Evaluating batches: 100%|| 125/125 [12:45<00:00,  6.13s/it]\n",
      "INFO:__main__:Base model accuracy: 0.06 (59/1000)\n",
      "INFO:__main__:Final base model accuracy: 0.06\n",
      "INFO:__main__:CPU memory usage: 2.16 GB\n",
      "INFO:__main__:GPU memory allocated: 8.26 GB\n",
      "INFO:__main__:GPU memory reserved: 14.72 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import logging\n",
    "import re\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to monitor memory usage\n",
    "def log_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    logger.info(f\"CPU memory usage: {mem_info.rss / 1e9:.2f} GB\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        logger.info(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# Validate example\n",
    "def is_valid_example(example):\n",
    "    if not isinstance(example, dict):\n",
    "        logger.warning(f\"Invalid example: not a dictionary - {example}\")\n",
    "        return False\n",
    "    prompt = example.get(\"problem\")\n",
    "    solution = example.get(\"solution\")\n",
    "    if prompt is None or solution is None:\n",
    "        logger.warning(f\"Invalid example: missing problem/solution - {example}\")\n",
    "        return False\n",
    "    if not isinstance(prompt, str) or not isinstance(solution, str):\n",
    "        logger.warning(f\"Invalid example: non-string problem/solution - {example}\")\n",
    "        return False\n",
    "    if len(prompt.strip()) < 10 or len(solution.strip()) < 10:\n",
    "        logger.warning(f\"Invalid example: empty/short problem/solution - {example}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Load evaluation dataset\n",
    "def load_eval_dataset(file_path=\"MATH dataset.xlsx\"):\n",
    "    logger.info(f\"Loading evaluation dataset from {file_path}...\")\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        data = df.to_dict(orient=\"records\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load Excel file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    valid_data = [item for item in data if is_valid_example(item)]\n",
    "    logger.info(f\"Loaded {len(data)} examples, filtered to {len(valid_data)} valid examples\")\n",
    "    return valid_data\n",
    "\n",
    "# Model and Tokenizer Setup\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Load tokenizer with left padding\n",
    "logger.info(f\"Loading tokenizer from {base_model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side=\"left\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    logger.info(f\"Set tokenizer pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Load base model on GPU with 4-bit quantization\n",
    "logger.info(\"Loading base model on GPU with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Verify model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Total parameters: {total_params}\")\n",
    "\n",
    "# Disable gradient checkpointing for evaluation (inference only)\n",
    "model.gradient_checkpointing_disable()\n",
    "log_memory_usage()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_base_model(data, max_samples=1000, batch_size=8):\n",
    "    logger.info(f\"Evaluating base model on {max_samples} samples with batch_size={batch_size}...\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    num_samples = min(max_samples, len(data))\n",
    "\n",
    "    # Process in batches\n",
    "    for batch_start in tqdm(range(0, num_samples, batch_size), desc=\"Evaluating batches\"):\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "        batch_data = data[batch_start:batch_end]\n",
    "\n",
    "        # Prepare batch inputs\n",
    "        prompts = [example[\"problem\"] for example in batch_data]\n",
    "        expected_solutions = [example[\"solution\"] for example in batch_data]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(\"cuda:0\")\n",
    "\n",
    "        # Generate responses for batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,  # Suitable for MATH problems\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode generated outputs\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "        # Extract answers (supports CoT)\n",
    "        def extract_answer(text):\n",
    "            # Try LaTeX boxed answer\n",
    "            boxed = re.search(r'\\\\boxed\\{([^}]+)\\}', text)\n",
    "            if boxed:\n",
    "                return boxed.group(1).strip()\n",
    "            # Try final answer pattern\n",
    "            final_answer = re.search(r'(?:Final Answer|Answer):?\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "            if final_answer:\n",
    "                return final_answer.group(1).strip()\n",
    "            # Try last number\n",
    "            numbers = re.findall(r'\\d+', text)\n",
    "            return numbers[-1] if numbers else \"\"\n",
    "\n",
    "        # Evaluate batch\n",
    "        for idx, (generated, expected_solution) in enumerate(zip(generated_texts, expected_solutions)):\n",
    "            global_idx = batch_start + idx\n",
    "            generated_answer = extract_answer(generated)\n",
    "            expected_answer = extract_answer(expected_solution)\n",
    "\n",
    "            # Log for debugging\n",
    "            logger.debug(f\"Example {global_idx}: Prompt: {prompts[idx][:100]}...\")\n",
    "            logger.debug(f\"Expected Solution: {expected_solution[:200]}...\")\n",
    "            logger.debug(f\"Expected Answer: {expected_answer}\")\n",
    "            logger.debug(f\"Generated: {generated[:200]}...\")\n",
    "            logger.debug(f\"Generated Answer: {generated_answer}\")\n",
    "\n",
    "            # Compare answers\n",
    "            if generated_answer == expected_answer:\n",
    "                correct += 1\n",
    "\n",
    "        # Log memory usage periodically\n",
    "        if (batch_start // batch_size + 1) % 10 == 0:\n",
    "            log_memory_usage()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = correct / num_samples if num_samples > 0 else 0\n",
    "    logger.info(f\"Base model accuracy: {accuracy:.2f} ({correct}/{num_samples})\")\n",
    "    return accuracy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    eval_data = load_eval_dataset(\"MATH dataset.xlsx\")\n",
    "\n",
    "    # Evaluate\n",
    "    base_accuracy = evaluate_base_model(eval_data, max_samples=1000, batch_size=8)\n",
    "    logger.info(f\"Final base model accuracy: {base_accuracy:.2f}\")\n",
    "    log_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b6c32-9990-49da-96e1-ab40dbb87527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
